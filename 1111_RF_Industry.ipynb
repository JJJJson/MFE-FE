{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7d19622",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.4\n",
      "  latest version: 23.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.11.0\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/jingzhao/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - pandas\n",
      "\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  pandas                              2.0.3-py311hdb55bb0_0 --> 2.1.4-py311hdb55bb0_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3d2c221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.4\n",
      "  latest version: 23.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.11.0\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e6ce7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.4\n",
      "  latest version: 23.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.11.0\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f85e283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "industry_portfolio_file = '/Users/jingzhao/Desktop/FE PW1/Data/17 Industry Portfolios.CSV'\n",
    "financial_data_file = '/Users/jingzhao/Desktop/FE PW1/Data/Financial Data.CSV'\n",
    "\n",
    "industry_portfolio_data = pd.read_csv(industry_portfolio_file)\n",
    "financial_data = pd.read_csv(financial_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4bb54a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Trans</td>\n",
       "      <td>0.217498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Steel</td>\n",
       "      <td>0.099219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Machn</td>\n",
       "      <td>0.091429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Finan</td>\n",
       "      <td>0.064864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Other</td>\n",
       "      <td>0.062537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature  Importance\n",
       "12   Trans    0.217498\n",
       "8    Steel    0.099219\n",
       "10   Machn    0.091429\n",
       "15   Finan    0.064864\n",
       "16   Other    0.062537"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HML: Regressor Selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Data Construction\n",
    "merged_data = pd.merge(industry_portfolio_data, financial_data[['Date', 'HML']], on='Date')\n",
    "missing_values = merged_data.isnull().sum()\n",
    "\n",
    "# Preparing the data for the model\n",
    "X = merged_data.drop(['Date', 'HML'], axis=1)\n",
    "y = merged_data['HML']\n",
    "\n",
    "# Random Forest Model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Top 5 most important portfolios\n",
    "top_5_portfolios = feature_importances.head(5)\n",
    "top_5_portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "11a3b1a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HML</td>       <th>  R-squared (uncentered):</th>      <td>   0.487</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.484</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   175.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 06 Jan 2024</td> <th>  Prob (F-statistic):</th>          <td>2.37e-131</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:11:03</td>     <th>  Log-Likelihood:    </th>          <td> -2195.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   930</td>      <th>  AIC:               </th>          <td>   4401.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   925</td>      <th>  BIC:               </th>          <td>   4425.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Trans</th> <td>    0.2803</td> <td>    0.026</td> <td>   10.963</td> <td> 0.000</td> <td>    0.230</td> <td>    0.331</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Steel</th> <td>    0.1721</td> <td>    0.018</td> <td>    9.318</td> <td> 0.000</td> <td>    0.136</td> <td>    0.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Machn</th> <td>   -0.2657</td> <td>    0.027</td> <td>   -9.777</td> <td> 0.000</td> <td>   -0.319</td> <td>   -0.212</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Finan</th> <td>    0.3271</td> <td>    0.027</td> <td>   12.266</td> <td> 0.000</td> <td>    0.275</td> <td>    0.379</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Other</th> <td>   -0.5043</td> <td>    0.039</td> <td>  -12.929</td> <td> 0.000</td> <td>   -0.581</td> <td>   -0.428</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>100.771</td> <th>  Durbin-Watson:     </th> <td>   1.915</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 348.924</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.488</td>  <th>  Prob(JB):          </th> <td>1.71e-76</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.838</td>  <th>  Cond. No.          </th> <td>    7.64</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] R² is computed without centering (uncentered) since the model does not contain a constant.<br/>[2] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       HML        & \\textbf{  R-squared (uncentered):}      &     0.487   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared (uncentered):} &     0.484   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       }          &     175.6   \\\\\n",
       "\\textbf{Date:}             & Sat, 06 Jan 2024 & \\textbf{  Prob (F-statistic):}          & 2.37e-131   \\\\\n",
       "\\textbf{Time:}             &     10:11:03     & \\textbf{  Log-Likelihood:    }          &   -2195.3   \\\\\n",
       "\\textbf{No. Observations:} &         930      & \\textbf{  AIC:               }          &     4401.   \\\\\n",
       "\\textbf{Df Residuals:}     &         925      & \\textbf{  BIC:               }          &     4425.   \\\\\n",
       "\\textbf{Df Model:}         &           5      & \\textbf{                     }          &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     }          &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Trans} &       0.2803  &        0.026     &    10.963  &         0.000        &        0.230    &        0.331     \\\\\n",
       "\\textbf{Steel} &       0.1721  &        0.018     &     9.318  &         0.000        &        0.136    &        0.208     \\\\\n",
       "\\textbf{Machn} &      -0.2657  &        0.027     &    -9.777  &         0.000        &       -0.319    &       -0.212     \\\\\n",
       "\\textbf{Finan} &       0.3271  &        0.027     &    12.266  &         0.000        &        0.275    &        0.379     \\\\\n",
       "\\textbf{Other} &      -0.5043  &        0.039     &   -12.929  &         0.000        &       -0.581    &       -0.428     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 100.771 & \\textbf{  Durbin-Watson:     } &    1.915  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } &  348.924  \\\\\n",
       "\\textbf{Skew:}          &   0.488 & \\textbf{  Prob(JB):          } & 1.71e-76  \\\\\n",
       "\\textbf{Kurtosis:}      &   5.838 & \\textbf{  Cond. No.          } &     7.64  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] R² is computed without centering (uncentered) since the model does not contain a constant. \\newline\n",
       " [2] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:                    HML   R-squared (uncentered):                   0.487\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              0.484\n",
       "Method:                 Least Squares   F-statistic:                              175.6\n",
       "Date:                Sat, 06 Jan 2024   Prob (F-statistic):                   2.37e-131\n",
       "Time:                        10:11:03   Log-Likelihood:                         -2195.3\n",
       "No. Observations:                 930   AIC:                                      4401.\n",
       "Df Residuals:                     925   BIC:                                      4425.\n",
       "Df Model:                           5                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Trans          0.2803      0.026     10.963      0.000       0.230       0.331\n",
       "Steel          0.1721      0.018      9.318      0.000       0.136       0.208\n",
       "Machn         -0.2657      0.027     -9.777      0.000      -0.319      -0.212\n",
       "Finan          0.3271      0.027     12.266      0.000       0.275       0.379\n",
       "Other         -0.5043      0.039    -12.929      0.000      -0.581      -0.428\n",
       "==============================================================================\n",
       "Omnibus:                      100.771   Durbin-Watson:                   1.915\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              348.924\n",
       "Skew:                           0.488   Prob(JB):                     1.71e-76\n",
       "Kurtosis:                       5.838   Cond. No.                         7.64\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
       "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HML: Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "selected_features = top_5_portfolios['Feature'].tolist()\n",
    "\n",
    "# Preparing the data with selected features\n",
    "X_selected = merged_data[selected_features]\n",
    "\n",
    "# Splitting the data into training and testing sets for the selected features\n",
    "X_train_selected, X_test_selected, y_train_selected, y_test_selected = train_test_split(\n",
    "    X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating and fitting the Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_selected, y_train_selected)\n",
    "\n",
    "# Making predictions and evaluating the model\n",
    "y_pred_selected = lr_model.predict(X_test_selected)\n",
    "mse_selected = mean_squared_error(y_test_selected, y_pred_selected)\n",
    "rmse_selected = np.sqrt(mse_selected)\n",
    "\n",
    "# Output the model performance\n",
    "mse_selected, rmse_selected\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Creating and fitting the OLS model\n",
    "ols_model = sm.OLS(y_train_selected, X_train_selected).fit()\n",
    "\n",
    "# Printing the summary of the OLS model\n",
    "ols_summary = ols_model.summary()\n",
    "ols_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8db542f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HML Rolling Regression\n",
    "from datetime import timedelta\n",
    "def rolling_regression(data, train_years, test_years):\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%Y%m')\n",
    "\n",
    "    unique_years = data['Date'].dt.year.unique()\n",
    "    \n",
    "    results = []\n",
    "    start_year = unique_years[0]\n",
    "\n",
    "    # Perform rolling regression\n",
    "    while start_year + train_years + test_years <= unique_years[-1]:\n",
    "        # Define training and testing periods\n",
    "        train_start = pd.Timestamp(year=start_year, month=1, day=1)\n",
    "        train_end = train_start + timedelta(days=365 * train_years)\n",
    "        test_end = train_end + timedelta(days=365 * test_years)\n",
    "\n",
    "        # Subset the data for training and testing\n",
    "        train_data = data[(data['Date'] >= train_start) & (data['Date'] < train_end)]\n",
    "        test_data = data[(data['Date'] >= train_end) & (data['Date'] < test_end)]\n",
    "\n",
    "        # Fit the model\n",
    "        X_train = train_data[selected_features]\n",
    "        y_train = train_data['HML']\n",
    "        model = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "        # Predict on test data\n",
    "        X_test = test_data[selected_features]\n",
    "        y_test = test_data['HML']\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Collect coefficients and predictions\n",
    "        coefficients = model.coef_\n",
    "        results.append({\n",
    "            'train_start': train_start,\n",
    "            'train_end': train_end,\n",
    "            'test_end': test_end,\n",
    "            'coefficients': coefficients,\n",
    "            'predicted_HML': y_pred\n",
    "        })\n",
    "\n",
    "        # Move to the next period\n",
    "        start_year += test_years\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Rolling regression for each scenario\n",
    "results_5_year = rolling_regression(merged_data, train_years=5, test_years=5)\n",
    "results_10_year = rolling_regression(merged_data, train_years=10, test_years=5)\n",
    "results_20_year = rolling_regression(merged_data, train_years=20, test_years=5)\n",
    "\n",
    "results_5_year.head(), results_10_year.head(), results_20_year.head()\n",
    "# Extracting betas and predicted HML into separate dataframes for each time scheme\n",
    "\n",
    "def extract_betas_and_predictions(results):\n",
    "    betas = []\n",
    "    predicted_HML = []\n",
    "\n",
    "    for index, row in results.iterrows():\n",
    "        betas.append({\n",
    "            'train_start': row['train_start'],\n",
    "            'train_end': row['train_end'],\n",
    "            'test_end': row['test_end'],\n",
    "            'Trans_beta': row['coefficients'][0],\n",
    "            'Steel_beta': row['coefficients'][1],\n",
    "            'Machn_beta': row['coefficients'][2],\n",
    "            'Finan_beta': row['coefficients'][3],\n",
    "            'Other_beta': row['coefficients'][4]\n",
    "        })\n",
    "        predicted_HML.extend(row['predicted_HML'])\n",
    "\n",
    "    betas_df = pd.DataFrame(betas)\n",
    "    predicted_HML_df = pd.DataFrame({'predicted_HML': predicted_HML})\n",
    "\n",
    "    return betas_df, predicted_HML_df\n",
    "\n",
    "# Extracting for each time scheme\n",
    "betas_5_year, predicted_HML_5_year = extract_betas_and_predictions(results_5_year)\n",
    "betas_10_year, predicted_HML_10_year = extract_betas_and_predictions(results_10_year)\n",
    "betas_20_year, predicted_HML_20_year = extract_betas_and_predictions(results_20_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89303bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(         residuals\n",
       " count  1080.000000\n",
       " mean     -0.031597\n",
       " std       2.475316\n",
       " min     -13.710342\n",
       " 25%      -1.332233\n",
       " 50%      -0.075307\n",
       " 75%       1.360415\n",
       " max      10.027822,\n",
       "          residuals\n",
       " count  1020.000000\n",
       " mean     -0.098707\n",
       " std       2.148893\n",
       " min     -13.030419\n",
       " 25%      -1.326080\n",
       " 50%      -0.103802\n",
       " 75%       1.141364\n",
       " max       9.186277,\n",
       "         residuals\n",
       " count  900.000000\n",
       " mean    -0.130205\n",
       " std      2.086146\n",
       " min     -8.466385\n",
       " 25%     -1.372565\n",
       " 50%     -0.163566\n",
       " 75%      1.103427\n",
       " max      8.381515)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Residual\n",
    "def calculate_residuals(data, predicted_HML, train_years, test_years):\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%Y%m')\n",
    "    unique_years = data['Date'].dt.year.unique()\n",
    "    residuals = []\n",
    "\n",
    "    start_year = unique_years[0]\n",
    "    predicted_index = 0\n",
    "\n",
    "    while start_year + train_years + test_years <= unique_years[-1]:\n",
    "        # Define the testing period\n",
    "        test_start = pd.Timestamp(year=start_year + train_years, month=1, day=1)\n",
    "        test_end = test_start + timedelta(days=365 * test_years)\n",
    "\n",
    "        # Subset the actual data for the testing period\n",
    "        test_data = data[(data['Date'] >= test_start) & (data['Date'] < test_end)]\n",
    "        actual_HML = test_data['HML'].values\n",
    "\n",
    "        # Calculate residuals\n",
    "        predicted_HML_values = predicted_HML['predicted_HML'].iloc[predicted_index:predicted_index + len(actual_HML)]\n",
    "        residual = actual_HML - predicted_HML_values\n",
    "        residuals.extend(residual)\n",
    "\n",
    "        # Update indices\n",
    "        predicted_index += len(actual_HML)\n",
    "        start_year += test_years\n",
    "\n",
    "    return pd.DataFrame({'residuals': residuals})\n",
    "\n",
    "# Calculate residuals for each rolling scheme\n",
    "residuals_5_year = calculate_residuals(merged_data, predicted_HML_5_year, 5, 5)\n",
    "residuals_10_year = calculate_residuals(merged_data, predicted_HML_10_year, 10, 5)\n",
    "residuals_20_year = calculate_residuals(merged_data, predicted_HML_20_year, 20, 5)\n",
    "\n",
    "residual_summary_5_year = residuals_5_year.describe()\n",
    "residual_summary_10_year = residuals_10_year.describe()\n",
    "residual_summary_20_year = residuals_20_year.describe()\n",
    "\n",
    "residual_summary_5_year, residual_summary_10_year, residual_summary_20_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "89d794fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6612.31704619941, 4715.415162630034, 3927.7100470840132)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SSE for each time scheme\n",
    "sse_5_year = np.sum(residuals_5_year['residuals'] ** 2)\n",
    "sse_10_year = np.sum(residuals_10_year['residuals'] ** 2)\n",
    "sse_20_year = np.sum(residuals_20_year['residuals'] ** 2)\n",
    "\n",
    "sse_5_year, sse_10_year, sse_20_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "10f456d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4973857240612507, 0.42530172773361963, 0.4024258908893674)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out-of-sample R-squared\n",
    "def calculate_out_of_sample_r_squared(data, predicted_HML, residuals, train_years, test_years):\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%Y%m')\n",
    "    unique_years = data['Date'].dt.year.unique()\n",
    "\n",
    "    total_sum_squares = 0\n",
    "    residual_sum_squares = 0\n",
    "    predicted_index = 0\n",
    "\n",
    "    start_year = unique_years[0]\n",
    "\n",
    "    while start_year + train_years + test_years <= unique_years[-1]:\n",
    "        # Define the testing period\n",
    "        test_start = pd.Timestamp(year=start_year + train_years, month=1, day=1)\n",
    "        test_end = test_start + timedelta(days=365 * test_years)\n",
    "\n",
    "        # Subset the actual data for the testing period\n",
    "        test_data = data[(data['Date'] >= test_start) & (data['Date'] < test_end)]\n",
    "        actual_HML = test_data['HML'].values\n",
    "\n",
    "        # Calculate total sum of squares and residual sum of squares\n",
    "        mean_actual_HML = np.mean(actual_HML)\n",
    "        total_sum_squares += np.sum((actual_HML - mean_actual_HML) ** 2)\n",
    "        residual_sum_squares += np.sum(residuals['residuals'].iloc[predicted_index:predicted_index + len(actual_HML)] ** 2)\n",
    "\n",
    "        # Update indices\n",
    "        predicted_index += len(actual_HML)\n",
    "        start_year += test_years\n",
    "\n",
    "    return 1 - (residual_sum_squares / total_sum_squares)\n",
    "\n",
    "# Calculate out-of-sample R-squared for each rolling scheme\n",
    "r_squared_5_year = calculate_out_of_sample_r_squared(merged_data, predicted_HML_5_year, residuals_5_year, 5, 5)\n",
    "r_squared_10_year = calculate_out_of_sample_r_squared(merged_data, predicted_HML_10_year, residuals_10_year, 10, 5)\n",
    "r_squared_20_year = calculate_out_of_sample_r_squared(merged_data, predicted_HML_20_year, residuals_20_year, 20, 5)\n",
    "\n",
    "r_squared_5_year, r_squared_10_year, r_squared_20_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e7309706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Trans</td>\n",
       "      <td>0.162974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FabPr</td>\n",
       "      <td>0.103529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Machn</td>\n",
       "      <td>0.087123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Finan</td>\n",
       "      <td>0.081107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Steel</td>\n",
       "      <td>0.080031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature  Importance\n",
       "12   Trans    0.162974\n",
       "9    FabPr    0.103529\n",
       "10   Machn    0.087123\n",
       "15   Finan    0.081107\n",
       "8    Steel    0.080031"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MOM Get Regressors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "merged_data = pd.merge(industry_portfolio_data, financial_data[['Date', 'MOM']], on='Date')\n",
    "missing_values = merged_data.isnull().sum()\n",
    "X = merged_data.drop(['Date', 'MOM'], axis=1)\n",
    "y = merged_data['MOM']\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating and fitting the Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions and evaluating the model\n",
    "y_pred = rf_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Output the model performance and the missing values information\n",
    "mse, rmse, missing_values\n",
    "# Getting feature importances from the Random Forest model\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Top 5 most important portfolios\n",
    "top_5_portfolios = feature_importances.head(5)\n",
    "top_5_portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4ea0230c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>MOM</td>       <th>  R-squared (uncentered):</th>      <td>   0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.213</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   51.23</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 06 Jan 2024</td> <th>  Prob (F-statistic):</th>          <td>6.09e-47</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:13:15</td>     <th>  Log-Likelihood:    </th>          <td> -2642.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   930</td>      <th>  AIC:               </th>          <td>   5296.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   925</td>      <th>  BIC:               </th>          <td>   5320.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>              <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Trans</th> <td>   -0.1659</td> <td>    0.042</td> <td>   -3.956</td> <td> 0.000</td> <td>   -0.248</td> <td>   -0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FabPr</th> <td>    0.1316</td> <td>    0.044</td> <td>    3.018</td> <td> 0.003</td> <td>    0.046</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Machn</th> <td>    0.1856</td> <td>    0.039</td> <td>    4.728</td> <td> 0.000</td> <td>    0.109</td> <td>    0.263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Finan</th> <td>   -0.2328</td> <td>    0.041</td> <td>   -5.715</td> <td> 0.000</td> <td>   -0.313</td> <td>   -0.153</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Steel</th> <td>   -0.1492</td> <td>    0.030</td> <td>   -4.902</td> <td> 0.000</td> <td>   -0.209</td> <td>   -0.089</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>311.184</td> <th>  Durbin-Watson:     </th> <td>   1.775</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2831.555</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-1.258</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>11.169</td>  <th>  Cond. No.          </th> <td>    5.56</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] R² is computed without centering (uncentered) since the model does not contain a constant.<br/>[2] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       MOM        & \\textbf{  R-squared (uncentered):}      &     0.217   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared (uncentered):} &     0.213   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       }          &     51.23   \\\\\n",
       "\\textbf{Date:}             & Sat, 06 Jan 2024 & \\textbf{  Prob (F-statistic):}          &  6.09e-47   \\\\\n",
       "\\textbf{Time:}             &     10:13:15     & \\textbf{  Log-Likelihood:    }          &   -2642.8   \\\\\n",
       "\\textbf{No. Observations:} &         930      & \\textbf{  AIC:               }          &     5296.   \\\\\n",
       "\\textbf{Df Residuals:}     &         925      & \\textbf{  BIC:               }          &     5320.   \\\\\n",
       "\\textbf{Df Model:}         &           5      & \\textbf{                     }          &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     }          &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Trans} &      -0.1659  &        0.042     &    -3.956  &         0.000        &       -0.248    &       -0.084     \\\\\n",
       "\\textbf{FabPr} &       0.1316  &        0.044     &     3.018  &         0.003        &        0.046    &        0.217     \\\\\n",
       "\\textbf{Machn} &       0.1856  &        0.039     &     4.728  &         0.000        &        0.109    &        0.263     \\\\\n",
       "\\textbf{Finan} &      -0.2328  &        0.041     &    -5.715  &         0.000        &       -0.313    &       -0.153     \\\\\n",
       "\\textbf{Steel} &      -0.1492  &        0.030     &    -4.902  &         0.000        &       -0.209    &       -0.089     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 311.184 & \\textbf{  Durbin-Watson:     } &    1.775  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2831.555  \\\\\n",
       "\\textbf{Skew:}          &  -1.258 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  11.169 & \\textbf{  Cond. No.          } &     5.56  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] R² is computed without centering (uncentered) since the model does not contain a constant. \\newline\n",
       " [2] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:                    MOM   R-squared (uncentered):                   0.217\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              0.213\n",
       "Method:                 Least Squares   F-statistic:                              51.23\n",
       "Date:                Sat, 06 Jan 2024   Prob (F-statistic):                    6.09e-47\n",
       "Time:                        10:13:15   Log-Likelihood:                         -2642.8\n",
       "No. Observations:                 930   AIC:                                      5296.\n",
       "Df Residuals:                     925   BIC:                                      5320.\n",
       "Df Model:                           5                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Trans         -0.1659      0.042     -3.956      0.000      -0.248      -0.084\n",
       "FabPr          0.1316      0.044      3.018      0.003       0.046       0.217\n",
       "Machn          0.1856      0.039      4.728      0.000       0.109       0.263\n",
       "Finan         -0.2328      0.041     -5.715      0.000      -0.313      -0.153\n",
       "Steel         -0.1492      0.030     -4.902      0.000      -0.209      -0.089\n",
       "==============================================================================\n",
       "Omnibus:                      311.184   Durbin-Watson:                   1.775\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2831.555\n",
       "Skew:                          -1.258   Prob(JB):                         0.00\n",
       "Kurtosis:                      11.169   Cond. No.                         5.56\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
       "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MOM: Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "selected_features = top_5_portfolios['Feature'].tolist()\n",
    "X_selected = merged_data[selected_features]\n",
    "\n",
    "# Splitting data\n",
    "X_train_selected, X_test_selected, y_train_selected, y_test_selected = train_test_split(\n",
    "    X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating and fitting the Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_selected, y_train_selected)\n",
    "\n",
    "# Making predictions and evaluating the model\n",
    "y_pred_selected = lr_model.predict(X_test_selected)\n",
    "mse_selected = mean_squared_error(y_test_selected, y_pred_selected)\n",
    "rmse_selected = np.sqrt(mse_selected)\n",
    "\n",
    "# Output the model performance\n",
    "mse_selected, rmse_selected\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Creating and fitting the OLS model\n",
    "ols_model = sm.OLS(y_train_selected, X_train_selected).fit()\n",
    "\n",
    "# Printing the summary of the OLS model\n",
    "ols_summary = ols_model.summary()\n",
    "ols_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a3caf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOM Rolling Regression\n",
    "from datetime import timedelta\n",
    "\n",
    "def rolling_regression(data, train_years, test_years):\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%Y%m')\n",
    "    unique_years = data['Date'].dt.year.unique()\n",
    "    \n",
    "    results = []\n",
    "    start_year = unique_years[0]\n",
    "\n",
    "    # Perform rolling regression\n",
    "    while start_year + train_years + test_years <= unique_years[-1]:\n",
    "        train_start = pd.Timestamp(year=start_year, month=1, day=1)\n",
    "        train_end = train_start + timedelta(days=365 * train_years)\n",
    "        test_end = train_end + timedelta(days=365 * test_years)\n",
    "\n",
    "        train_data = data[(data['Date'] >= train_start) & (data['Date'] < train_end)]\n",
    "        test_data = data[(data['Date'] >= train_end) & (data['Date'] < test_end)]\n",
    "\n",
    "        # Fit the model\n",
    "        X_train = train_data[selected_features]\n",
    "        y_train = train_data['MOM']\n",
    "        model = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "        # Predict on test data\n",
    "        X_test = test_data[selected_features]\n",
    "        y_test = test_data['MOM']\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Collect coefficients and predictions\n",
    "        coefficients = model.coef_\n",
    "        results.append({\n",
    "            'train_start': train_start,\n",
    "            'train_end': train_end,\n",
    "            'test_end': test_end,\n",
    "            'coefficients': coefficients,\n",
    "            'predicted_MOM': y_pred\n",
    "        })\n",
    "\n",
    "        # Move to the next period\n",
    "        start_year += test_years\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Rolling regression for each scenario\n",
    "results_5_year = rolling_regression(merged_data, train_years=5, test_years=5)\n",
    "results_10_year = rolling_regression(merged_data, train_years=10, test_years=5)\n",
    "results_20_year = rolling_regression(merged_data, train_years=20, test_years=5)\n",
    "\n",
    "results_5_year.head(), results_10_year.head(), results_20_year.head()\n",
    "\n",
    "def extract_betas_and_predictions(results):\n",
    "    betas = []\n",
    "    predicted_MOM = []\n",
    "\n",
    "    for index, row in results.iterrows():\n",
    "        betas.append({\n",
    "            'train_start': row['train_start'],\n",
    "            'train_end': row['train_end'],\n",
    "            'test_end': row['test_end'],\n",
    "            'Trans_beta': row['coefficients'][0],\n",
    "            'Steel_beta': row['coefficients'][1],\n",
    "            'Machn_beta': row['coefficients'][2],\n",
    "            'Finan_beta': row['coefficients'][3],\n",
    "            'Other_beta': row['coefficients'][4]\n",
    "        })\n",
    "        predicted_MOM.extend(row['predicted_MOM'])\n",
    "\n",
    "    betas_df = pd.DataFrame(betas)\n",
    "    predicted_MOM_df = pd.DataFrame({'predicted_MOM': predicted_MOM})\n",
    "\n",
    "    return betas_df, predicted_MOM_df\n",
    "\n",
    "# Extracting for each time scheme\n",
    "betas_5_year, predicted_MOM_5_year = extract_betas_and_predictions(results_5_year)\n",
    "betas_10_year, predicted_MOM_10_year = extract_betas_and_predictions(results_10_year)\n",
    "betas_20_year, predicted_MOM_20_year = extract_betas_and_predictions(results_20_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fdcafb87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   residuals\n",
       " 0  -3.057034\n",
       " 1   5.482401\n",
       " 2   0.420145\n",
       " 3   0.416680\n",
       " 4  -2.716594,\n",
       "    residuals\n",
       " 0  -5.889969\n",
       " 1   7.021825\n",
       " 2   4.742564\n",
       " 3 -10.277711\n",
       " 4  -3.687156,\n",
       "    residuals\n",
       " 0  -8.271724\n",
       " 1  -1.951298\n",
       " 2   1.862592\n",
       " 3   1.541931\n",
       " 4   1.127191)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Residual\n",
    "def calculate_residuals(data, predicted_MOM, train_years, test_years):\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%Y%m')\n",
    "    unique_years = data['Date'].dt.year.unique()\n",
    "    residuals = []\n",
    "\n",
    "    start_year = unique_years[0]\n",
    "    predicted_index = 0\n",
    "\n",
    "    while start_year + train_years + test_years <= unique_years[-1]:\n",
    "        # Define the testing period\n",
    "        test_start = pd.Timestamp(year=start_year + train_years, month=1, day=1)\n",
    "        test_end = test_start + timedelta(days=365 * test_years)\n",
    "\n",
    "        # Subset the actual data for the testing period\n",
    "        test_data = data[(data['Date'] >= test_start) & (data['Date'] < test_end)]\n",
    "        actual_MOM = test_data['MOM'].values\n",
    "\n",
    "        # Calculate residuals\n",
    "        predicted_MOM_values = predicted_MOM['predicted_MOM'].iloc[predicted_index:predicted_index + len(actual_MOM)]\n",
    "        residual = actual_MOM - predicted_MOM_values\n",
    "        residuals.extend(residual)\n",
    "\n",
    "        # Update indices\n",
    "        predicted_index += len(actual_MOM)\n",
    "        start_year += test_years\n",
    "\n",
    "    return pd.DataFrame({'residuals': residuals})\n",
    "\n",
    "# Calculate residuals for each rolling scheme\n",
    "residuals_5_year = calculate_residuals(merged_data, predicted_MOM_5_year, 5, 5)\n",
    "residuals_10_year = calculate_residuals(merged_data, predicted_MOM_10_year, 10, 5)\n",
    "residuals_20_year = calculate_residuals(merged_data, predicted_MOM_20_year, 20, 5)\n",
    "\n",
    "residuals_5_year.head(), residuals_10_year.head(), residuals_20_year.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "af169f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(         residuals\n",
       " count  1080.000000\n",
       " mean     -0.031597\n",
       " std       2.475316\n",
       " min     -13.710342\n",
       " 25%      -1.332233\n",
       " 50%      -0.075307\n",
       " 75%       1.360415\n",
       " max      10.027822,\n",
       "          residuals\n",
       " count  1020.000000\n",
       " mean     -0.098707\n",
       " std       2.148893\n",
       " min     -13.030419\n",
       " 25%      -1.326080\n",
       " 50%      -0.103802\n",
       " 75%       1.141364\n",
       " max       9.186277,\n",
       "         residuals\n",
       " count  900.000000\n",
       " mean    -0.130205\n",
       " std      2.086146\n",
       " min     -8.466385\n",
       " 25%     -1.372565\n",
       " 50%     -0.163566\n",
       " 75%      1.103427\n",
       " max      8.381515)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Residual Summary\n",
    "residual_summary_5_year = residuals_5_year.describe()\n",
    "residual_summary_10_year = residuals_10_year.describe()\n",
    "residual_summary_20_year = residuals_20_year.describe()\n",
    "\n",
    "residual_summary_5_year, residual_summary_10_year, residual_summary_20_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c685b34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6612.31704619941, 4715.415162630034, 3927.7100470840132)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SSE for each time scheme\n",
    "sse_5_year = np.sum(residuals_5_year['residuals'] ** 2)\n",
    "sse_10_year = np.sum(residuals_10_year['residuals'] ** 2)\n",
    "sse_20_year = np.sum(residuals_20_year['residuals'] ** 2)\n",
    "\n",
    "sse_5_year, sse_10_year, sse_20_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3cbd33fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7104265785457922, 0.7125108563357591, 0.7074549705163533)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Out-of-sample R-squared\n",
    "def calculate_out_of_sample_r_squared(data, predicted_MOM, residuals, train_years, test_years):\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%Y%m')\n",
    "    unique_years = data['Date'].dt.year.unique()\n",
    "\n",
    "    total_sum_squares = 0\n",
    "    residual_sum_squares = 0\n",
    "    predicted_index = 0\n",
    "\n",
    "    start_year = unique_years[0]\n",
    "\n",
    "    while start_year + train_years + test_years <= unique_years[-1]:\n",
    "        # Define the testing period\n",
    "        test_start = pd.Timestamp(year=start_year + train_years, month=1, day=1)\n",
    "        test_end = test_start + timedelta(days=365 * test_years)\n",
    "\n",
    "        # Subset the actual data for the testing period\n",
    "        test_data = data[(data['Date'] >= test_start) & (data['Date'] < test_end)]\n",
    "        actual_MOM = test_data['MOM'].values\n",
    "\n",
    "        # Calculate total sum of squares and residual sum of squares\n",
    "        mean_actual_MOM = np.mean(actual_MOM)\n",
    "        total_sum_squares += np.sum((actual_MOM - mean_actual_MOM) ** 2)\n",
    "        residual_sum_squares += np.sum(residuals['residuals'].iloc[predicted_index:predicted_index + len(actual_MOM)] ** 2)\n",
    "\n",
    "        # Update indices\n",
    "        predicted_index += len(actual_MOM)\n",
    "        start_year += test_years\n",
    "\n",
    "    return 1 - (residual_sum_squares / total_sum_squares)\n",
    "\n",
    "# Calculate out-of-sample R-squared for each rolling scheme\n",
    "r_squared_5_year = calculate_out_of_sample_r_squared(merged_data, predicted_MOM_5_year, residuals_5_year, 5, 5)\n",
    "r_squared_10_year = calculate_out_of_sample_r_squared(merged_data, predicted_MOM_10_year, residuals_10_year, 10, 5)\n",
    "r_squared_20_year = calculate_out_of_sample_r_squared(merged_data, predicted_MOM_20_year, residuals_20_year, 20, 5)\n",
    "\n",
    "r_squared_5_year, r_squared_10_year, r_squared_20_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d9ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
