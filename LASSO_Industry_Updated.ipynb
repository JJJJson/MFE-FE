{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15aa2997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.4\n",
      "  latest version: 23.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.11.0\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.4\n",
      "  latest version: 23.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.11.0\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.4\n",
      "  latest version: 23.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.11.0\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install pandas\n",
    "%conda install statsmodels\n",
    "%conda install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c47828c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "industries_data = pd.read_csv('/Users/jingzhao/Desktop/FE PW1/Data/17 Industry Portfolios.CSV')\n",
    "financial_data = pd.read_csv('/Users/jingzhao/Desktop/FE PW1/Data/Financial Data.CSV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "08d7d4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Food ', 'Mines', 'Oil  ', 'Clths', 'Durbl', 'Chems', 'Cnsum', 'Cnstr',\n",
      "       'Steel', 'FabPr', 'Machn', 'Cars ', 'Trans', 'Utils', 'Rtail', 'Finan',\n",
      "       'Other'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# HML: Lasso coefficient\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "merged_data = pd.merge(industries_data, financial_data[['Date', 'HML']], on='Date')\n",
    "X = merged_data.drop(columns=['Date', 'HML'])\n",
    "y = merged_data['HML']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Using LASSO regression with cross-validation to find the best alpha\n",
    "lasso = LassoCV(cv=5, random_state=42)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Predicting and evaluating the model\n",
    "y_pred = lasso.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Extracting the coefficients and the selected features\n",
    "lasso_coefs = lasso.coef_\n",
    "selected_features = X.columns[np.abs(lasso_coefs) > 0]\n",
    "\n",
    "selected_features, mse, lasso.alpha_\n",
    "\n",
    "# Creating a DataFrame to display the coefficients and their corresponding features\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': lasso_coefs\n",
    "})\n",
    "\n",
    "# Filtering out the features with zero coefficients\n",
    "significant_coefficients = coefficients[np.abs(coefficients['Coefficient']) > 0]\n",
    "significant_coefficients.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1a92c9c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'HML'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'HML'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Rolling regression for each scenario\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m results_5_year \u001b[38;5;241m=\u001b[39m rolling_regression(merged_data, selected_features, train_years\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, test_years\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     53\u001b[0m results_10_year \u001b[38;5;241m=\u001b[39m rolling_regression(merged_data, selected_features, train_years\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, test_years\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     54\u001b[0m results_20_year \u001b[38;5;241m=\u001b[39m rolling_regression(merged_data, selected_features, train_years\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, test_years\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[151], line 29\u001b[0m, in \u001b[0;36mrolling_regression\u001b[0;34m(data, selected_features, train_years, test_years)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m     28\u001b[0m X_train \u001b[38;5;241m=\u001b[39m train_data[selected_features]\n\u001b[0;32m---> 29\u001b[0m y_train \u001b[38;5;241m=\u001b[39m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHML\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegression()\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Predict on test data\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3796\u001b[0m     ):\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'HML'"
     ]
    }
   ],
   "source": [
    "# HML Rolling Regression\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def rolling_regression(data, selected_features, train_years, test_years):\n",
    "    # Convert 'Date' to datetime for easier date manipulation\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%Y%m')\n",
    "\n",
    "    # Get the unique years in the dataset\n",
    "    unique_years = data['Date'].dt.year.unique()\n",
    "    \n",
    "    results = []\n",
    "    start_year = unique_years[0]\n",
    "\n",
    "    # Perform rolling regression\n",
    "    while start_year + train_years + test_years <= unique_years[-1]:\n",
    "        # Define training and testing periods\n",
    "        train_start = pd.Timestamp(year=start_year, month=1, day=1)\n",
    "        train_end = train_start + pd.DateOffset(years=train_years)\n",
    "        test_end = train_end + pd.DateOffset(years=test_years)\n",
    "\n",
    "        # Subset the data for training and testing\n",
    "        train_data = data[(data['Date'] >= train_start) & (data['Date'] < train_end)]\n",
    "        test_data = data[(data['Date'] >= train_end) & (data['Date'] < test_end)]\n",
    "\n",
    "        # Fit the model\n",
    "        X_train = train_data[selected_features]\n",
    "        y_train = train_data['HML']\n",
    "        model = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "        # Predict on test data\n",
    "        X_test = test_data[selected_features]\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Collect coefficients and predictions\n",
    "        coefficients = model.coef_\n",
    "        results.append({\n",
    "            'train_start': train_start,\n",
    "            'train_end': train_end,\n",
    "            'test_end': test_end,\n",
    "            'coefficients': coefficients,\n",
    "            'predicted_HML': y_pred\n",
    "        })\n",
    "\n",
    "        # Move to the next period\n",
    "        start_year += test_years\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Rolling regression for each scenario\n",
    "results_5_year = rolling_regression(merged_data, selected_features, train_years=5, test_years=5)\n",
    "results_10_year = rolling_regression(merged_data, selected_features, train_years=10, test_years=5)\n",
    "results_20_year = rolling_regression(merged_data, selected_features, train_years=20, test_years=5)\n",
    "\n",
    "print(model.coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ff2be530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta and HML\n",
    "def extract_betas_and_predictions(results):\n",
    "    betas = []\n",
    "    predicted_HML = []\n",
    "\n",
    "    for index, row in results.iterrows():\n",
    "        # Extract beta coefficients\n",
    "        betas.append({\n",
    "            'train_start': row['train_start'],\n",
    "            'train_end': row['train_end'],\n",
    "            'test_end': row['test_end'],\n",
    "            **{f'beta_{i}': coef for i, coef in enumerate(row['coefficients'])}\n",
    "        })\n",
    "        \n",
    "        # Extract predicted HML values\n",
    "        for prediction in row['predicted_HML']:\n",
    "            predicted_HML.append({\n",
    "                'test_end': row['test_end'],\n",
    "                'predicted_HML': prediction\n",
    "            })\n",
    "\n",
    "    betas_df = pd.DataFrame(betas)\n",
    "    predicted_HML_df = pd.DataFrame(predicted_HML)\n",
    "\n",
    "    return betas_df, predicted_HML_df\n",
    "\n",
    "# Process the results for each time scheme\n",
    "betas_5_year, predicted_HML_5_year = extract_betas_and_predictions(results_5_year)\n",
    "betas_10_year, predicted_HML_10_year = extract_betas_and_predictions(results_10_year)\n",
    "betas_20_year, predicted_HML_20_year = extract_betas_and_predictions(results_20_year)\n",
    "\n",
    "df = pd.DataFrame(betas_5_year)\n",
    "df.to_csv('/Users/jingzhao/Desktop/Lasso_HML_Beta_5yr.csv', index=False)\n",
    "df = pd.DataFrame(betas_10_year)\n",
    "df.to_csv('/Users/jingzhao/Desktop/Lasso_HML_Beta_10yr.csv', index=False)\n",
    "df = pd.DataFrame(betas_20_year)\n",
    "df.to_csv('/Users/jingzhao/Desktop/Lasso_HML_Beta_20yr.csv', index=False)\n",
    "df = pd.DataFrame(predicted_HML_5_year)\n",
    "df.to_csv('/Users/jingzhao/Desktop/Lasso_HML_5yr.csv', index=False)\n",
    "df = pd.DataFrame(predicted_HML_10_year)\n",
    "df.to_csv('/Users/jingzhao/Desktop/Lasso_HML_10yr.csv', index=False)\n",
    "df = pd.DataFrame(predicted_HML_20_year)\n",
    "df.to_csv('/Users/jingzhao/Desktop/Lasso_HML_20yr.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d461ef65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      residuals\n",
      "0      5.150271\n",
      "1     -2.417107\n",
      "2      1.244794\n",
      "3      3.351280\n",
      "4      5.916826\n",
      "...         ...\n",
      "1075   1.277399\n",
      "1076  -1.560172\n",
      "1077  -2.308562\n",
      "1078   3.377945\n",
      "1079   1.691241\n",
      "\n",
      "[1080 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Residual\n",
    "def calculate_residuals(data, predicted_HML, train_years, test_years):\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%Y%m')\n",
    "    unique_years = data['Date'].dt.year.unique()\n",
    "    residuals = []\n",
    "\n",
    "    start_year = unique_years[0]\n",
    "    predicted_index = 0\n",
    "\n",
    "    while start_year + train_years + test_years <= unique_years[-1]:\n",
    "        # Define the testing period\n",
    "        test_start = pd.Timestamp(year=start_year + train_years, month=1, day=1)\n",
    "        test_end = test_start + timedelta(days=365 * test_years)\n",
    "\n",
    "        # Subset the actual data for the testing period\n",
    "        test_data = data[(data['Date'] >= test_start) & (data['Date'] < test_end)]\n",
    "        actual_HML = test_data['HML'].values\n",
    "\n",
    "        # Calculate residuals\n",
    "        predicted_HML_values = predicted_HML['predicted_HML'].iloc[predicted_index:predicted_index + len(actual_HML)]\n",
    "        residual = actual_HML - predicted_HML_values\n",
    "        residuals.extend(residual)\n",
    "\n",
    "        # Update indices\n",
    "        predicted_index += len(actual_HML)\n",
    "        start_year += test_years\n",
    "\n",
    "    return pd.DataFrame({'residuals': residuals})\n",
    "\n",
    "# Calculate residuals for each rolling scheme\n",
    "residuals_5_year = calculate_residuals(merged_data, predicted_HML_5_year, 5, 5)\n",
    "residuals_10_year = calculate_residuals(merged_data, predicted_HML_10_year, 10, 5)\n",
    "residuals_20_year = calculate_residuals(merged_data, predicted_HML_20_year, 20, 5)\n",
    "print(residuals_5_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a787503e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(         residuals\n",
       " count  1080.000000\n",
       " mean     -0.144324\n",
       " std       2.627564\n",
       " min     -13.519619\n",
       " 25%      -1.541285\n",
       " 50%      -0.288517\n",
       " 75%       1.280076\n",
       " max      18.252588,\n",
       "          residuals\n",
       " count  1020.000000\n",
       " mean     -0.122225\n",
       " std       2.129064\n",
       " min     -10.531912\n",
       " 25%      -1.355265\n",
       " 50%      -0.202789\n",
       " 75%       1.045957\n",
       " max       9.826449,\n",
       "         residuals\n",
       " count  900.000000\n",
       " mean    -0.173789\n",
       " std      2.025429\n",
       " min     -8.039950\n",
       " 25%     -1.424272\n",
       " 50%     -0.282700\n",
       " 75%      0.934058\n",
       " max      6.785557)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Residual Summary\n",
    "residual_summary_5_year = residuals_5_year.describe()\n",
    "residual_summary_10_year = residuals_10_year.describe()\n",
    "residual_summary_20_year = residuals_20_year.describe()\n",
    "\n",
    "residual_summary_5_year, residual_summary_10_year, residual_summary_20_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "dd83d3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7472.013824415835, 4634.276112332735, 3715.206031934155)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SSE for each time scheme\n",
    "sse_5_year = np.sum(residuals_5_year['residuals'] ** 2)\n",
    "sse_10_year = np.sum(residuals_10_year['residuals'] ** 2)\n",
    "sse_20_year = np.sum(residuals_20_year['residuals'] ** 2)\n",
    "\n",
    "sse_5_year, sse_10_year, sse_20_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "411cd069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.84759175456176, 0.8779221245139004, 0.8818784218952873)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_out_of_sample_r_squared(data, predicted_HML, residuals, train_years, test_years):\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%Y%m')\n",
    "    unique_years = data['Date'].dt.year.unique()\n",
    "\n",
    "    total_sum_squares = 0\n",
    "    residual_sum_squares = 0\n",
    "    predicted_index = 0\n",
    "\n",
    "    for start_year in unique_years:\n",
    "        if start_year + train_years + test_years > unique_years[-1]:\n",
    "            break\n",
    "\n",
    "        # Define the testing period\n",
    "        test_start = pd.Timestamp(year=start_year + train_years, month=1, day=1)\n",
    "        test_end = test_start + timedelta(days=365 * test_years)\n",
    "\n",
    "        # Subset the actual data for the testing period\n",
    "        test_data = data[(data['Date'] >= test_start) & (data['Date'] < test_end)]\n",
    "        actual_HML = test_data['HML'].values\n",
    "\n",
    "        # Calculate total sum of squares and residual sum of squares\n",
    "        mean_actual_HML = np.mean(actual_HML)\n",
    "        total_sum_squares += np.sum((actual_HML - mean_actual_HML) ** 2)\n",
    "        residual_sum_squares += np.sum(residuals['residuals'].iloc[predicted_index:predicted_index + len(actual_HML)] ** 2)\n",
    "\n",
    "        # Update indices\n",
    "        predicted_index += len(actual_HML)\n",
    "\n",
    "    return 1 - (residual_sum_squares / total_sum_squares)\n",
    "\n",
    "# Calculate out-of-sample R-squared for each rolling scheme\n",
    "r_squared_5_year = calculate_out_of_sample_r_squared(merged_data, predicted_HML_5_year, residuals_5_year, 5, 5)\n",
    "r_squared_10_year = calculate_out_of_sample_r_squared(merged_data, predicted_HML_10_year, residuals_10_year, 10, 5)\n",
    "r_squared_20_year = calculate_out_of_sample_r_squared(merged_data, predicted_HML_20_year, residuals_20_year, 20, 5)\n",
    "r_squared_5_year, r_squared_10_year, r_squared_20_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "facb98e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Food ', 'Mines', 'Oil  ', 'Clths', 'Durbl', 'Cnsum', 'Cnstr', 'Steel',\n",
      "       'FabPr', 'Machn', 'Cars ', 'Trans', 'Rtail', 'Finan', 'Other'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# MOM Select Regressor\n",
    "merged_data = pd.merge(industries_data, financial_data[['Date', 'MOM']], on='Date')\n",
    "X = merged_data.drop(columns=['Date', 'MOM'])\n",
    "y = merged_data['MOM']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Using LASSO regression with cross-validation to find the best alpha\n",
    "lasso = LassoCV(cv=5, random_state=42)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Predicting and evaluating the model\n",
    "y_pred = lasso.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Extracting the coefficients and the selected features\n",
    "lasso_coefs = lasso.coef_\n",
    "selected_features = X.columns[np.abs(lasso_coefs) > 0]\n",
    "\n",
    "selected_features, mse, lasso.alpha_\n",
    "\n",
    "# Creating a DataFrame to display the coefficients and their corresponding features\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': lasso_coefs\n",
    "})\n",
    "\n",
    "# Filtering out the features with zero coefficients\n",
    "significant_coefficients = coefficients[np.abs(coefficients['Coefficient']) > 0]\n",
    "significant_coefficients.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "64d196a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOM Rolling Regression\n",
    "def rolling_regression(data, selected_features, train_years, test_years):\n",
    "    # Convert 'Date' to datetime for easier date manipulation\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%Y%m')\n",
    "\n",
    "    # Get the unique years in the dataset\n",
    "    unique_years = data['Date'].dt.year.unique()\n",
    "    \n",
    "    results = []\n",
    "    start_year = unique_years[0]\n",
    "\n",
    "    # Perform rolling regression\n",
    "    while start_year + train_years + test_years <= unique_years[-1]:\n",
    "        # Define training and testing periods\n",
    "        train_start = pd.Timestamp(year=start_year, month=1, day=1)\n",
    "        train_end = train_start + pd.DateOffset(years=train_years)\n",
    "        test_end = train_end + pd.DateOffset(years=test_years)\n",
    "\n",
    "        # Subset the data for training and testing\n",
    "        train_data = data[(data['Date'] >= train_start) & (data['Date'] < train_end)]\n",
    "        test_data = data[(data['Date'] >= train_end) & (data['Date'] < test_end)]\n",
    "\n",
    "        # Fit the model\n",
    "        X_train = train_data[selected_features]\n",
    "        y_train = train_data['MOM']\n",
    "        model = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "        # Predict on test data\n",
    "        X_test = test_data[selected_features]\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Collect coefficients and predictions\n",
    "        coefficients = model.coef_\n",
    "        results.append({\n",
    "            'train_start': train_start,\n",
    "            'train_end': train_end,\n",
    "            'test_end': test_end,\n",
    "            'coefficients': coefficients,\n",
    "            'predicted_MOM': y_pred\n",
    "        })\n",
    "\n",
    "        # Move to the next period\n",
    "        start_year += test_years\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Rolling regression for each scenario\n",
    "results_5_year = rolling_regression(merged_data, selected_features, train_years=5, test_years=5)\n",
    "results_10_year = rolling_regression(merged_data, selected_features, train_years=10, test_years=5)\n",
    "results_20_year = rolling_regression(merged_data, selected_features, train_years=20, test_years=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "866b6940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta and MOM\n",
    "def extract_betas_and_predictions(results):\n",
    "    betas = []\n",
    "    predicted_MOM = []\n",
    "\n",
    "    for index, row in results.iterrows():\n",
    "        # Extract beta coefficients\n",
    "        betas.append({\n",
    "            'train_start': row['train_start'],\n",
    "            'train_end': row['train_end'],\n",
    "            'test_end': row['test_end'],\n",
    "            **{f'beta_{i}': coef for i, coef in enumerate(row['coefficients'])}\n",
    "        })\n",
    "        \n",
    "        # Extract predicted MOM values\n",
    "        for prediction in row['predicted_MOM']:\n",
    "            predicted_MOM.append({\n",
    "                'test_end': row['test_end'],\n",
    "                'predicted_MOM': prediction\n",
    "            })\n",
    "\n",
    "    betas_df = pd.DataFrame(betas)\n",
    "    predicted_MOM_df = pd.DataFrame(predicted_MOM)\n",
    "\n",
    "    return betas_df, predicted_MOM_df\n",
    "\n",
    "# Process the results for each time scheme\n",
    "betas_5_year, predicted_MOM_5_year = extract_betas_and_predictions(results_5_year)\n",
    "betas_10_year, predicted_MOM_10_year = extract_betas_and_predictions(results_10_year)\n",
    "betas_20_year, predicted_MOM_20_year = extract_betas_and_predictions(results_20_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f020df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual\n",
    "def calculate_residuals(data, predicted_MOM, train_years, test_years):\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%Y%m')\n",
    "    unique_years = data['Date'].dt.year.unique()\n",
    "    residuals = []\n",
    "\n",
    "    start_year = unique_years[0]\n",
    "    predicted_index = 0\n",
    "\n",
    "    while start_year + train_years + test_years <= unique_years[-1]:\n",
    "        # Define the testing period\n",
    "        test_start = pd.Timestamp(year=start_year + train_years, month=1, day=1)\n",
    "        test_end = test_start + timedelta(days=365 * test_years)\n",
    "\n",
    "        # Subset the actual data for the testing period\n",
    "        test_data = data[(data['Date'] >= test_start) & (data['Date'] < test_end)]\n",
    "        actual_MOM = test_data['MOM'].values\n",
    "\n",
    "        # Calculate residuals\n",
    "        predicted_MOM_values = predicted_MOM['predicted_MOM'].iloc[predicted_index:predicted_index + len(actual_MOM)]\n",
    "        residual = actual_MOM - predicted_MOM_values\n",
    "        residuals.extend(residual)\n",
    "\n",
    "        # Update indices\n",
    "        predicted_index += len(actual_MOM)\n",
    "        start_year += test_years\n",
    "\n",
    "    return pd.DataFrame({'residuals': residuals})\n",
    "\n",
    "# Calculate residuals for each rolling scheme\n",
    "residuals_5_year = calculate_residuals(merged_data, predicted_MOM_5_year, 5, 5)\n",
    "residuals_10_year = calculate_residuals(merged_data, predicted_MOM_10_year, 10, 5)\n",
    "residuals_20_year = calculate_residuals(merged_data, predicted_MOM_20_year, 20, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "af055729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(         residuals\n",
       " count  1080.000000\n",
       " mean      0.488007\n",
       " std       4.920040\n",
       " min     -35.485383\n",
       " 25%      -1.745210\n",
       " 50%       0.716320\n",
       " 75%       3.334757\n",
       " max      16.416958,\n",
       "          residuals\n",
       " count  1020.000000\n",
       " mean      0.403584\n",
       " std       4.389237\n",
       " min     -25.236305\n",
       " 25%      -1.549850\n",
       " 50%       0.569789\n",
       " 75%       2.989516\n",
       " max      14.434734,\n",
       "         residuals\n",
       " count  900.000000\n",
       " mean     0.240018\n",
       " std      4.140386\n",
       " min    -26.982898\n",
       " 25%     -1.628540\n",
       " 50%      0.294478\n",
       " 75%      2.526104\n",
       " max     16.632041)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Residual summaries\n",
    "residual_summary_5_year = residuals_5_year.describe()\n",
    "residual_summary_10_year = residuals_10_year.describe()\n",
    "residual_summary_20_year = residuals_20_year.describe()\n",
    "\n",
    "residual_summary_5_year, residual_summary_10_year, residual_summary_20_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3bcbeab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26376.327655018493, 19797.583460893133, 15463.220625138372)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SSE for each time scheme\n",
    "sse_5_year = np.sum(residuals_5_year['residuals'] ** 2)\n",
    "sse_10_year = np.sum(residuals_10_year['residuals'] ** 2)\n",
    "sse_20_year = np.sum(residuals_20_year['residuals'] ** 2)\n",
    "\n",
    "sse_5_year, sse_10_year, sse_20_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "82fba789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7051126639018661, 0.7332057629178734, 0.7606087359070746)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def calculate_out_of_sample_r_squared(data, predicted_MOM, residuals, train_years, test_years):\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%Y%m')\n",
    "    unique_years = data['Date'].dt.year.unique()\n",
    "\n",
    "    total_sum_squares = 0\n",
    "    residual_sum_squares = 0\n",
    "    predicted_index = 0\n",
    "\n",
    "    for start_year in unique_years:\n",
    "        if start_year + train_years + test_years > unique_years[-1]:\n",
    "            break\n",
    "\n",
    "        # Define the testing period\n",
    "        test_start = pd.Timestamp(year=start_year + train_years, month=1, day=1)\n",
    "        test_end = test_start + timedelta(days=365 * test_years)\n",
    "\n",
    "        # Subset the actual data for the testing period\n",
    "        test_data = data[(data['Date'] >= test_start) & (data['Date'] < test_end)]\n",
    "        actual_MOM = test_data['MOM'].values\n",
    "\n",
    "        # Calculate total sum of squares and residual sum of squares\n",
    "        mean_actual_MOM = np.mean(actual_MOM)\n",
    "        total_sum_squares += np.sum((actual_MOM - mean_actual_MOM) ** 2)\n",
    "        residual_sum_squares += np.sum(residuals['residuals'].iloc[predicted_index:predicted_index + len(actual_MOM)] ** 2)\n",
    "\n",
    "        # Update indices\n",
    "        predicted_index += len(actual_MOM)\n",
    "\n",
    "    return 1 - (residual_sum_squares / total_sum_squares)\n",
    "\n",
    "# Calculate out-of-sample R-squared for each rolling scheme\n",
    "r_squared_5_year = calculate_out_of_sample_r_squared(merged_data, predicted_MOM_5_year, residuals_5_year, 5, 5)\n",
    "r_squared_10_year = calculate_out_of_sample_r_squared(merged_data, predicted_MOM_10_year, residuals_10_year, 10, 5)\n",
    "r_squared_20_year = calculate_out_of_sample_r_squared(merged_data, predicted_MOM_20_year, residuals_20_year, 20, 5)\n",
    "r_squared_5_year, r_squared_10_year, r_squared_20_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98146890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
